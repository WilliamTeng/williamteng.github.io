<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"williamteng.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="基于人类反馈的强化学习（RLHF，Reinforcement Learning from Human Feedback）是人工智能（AI）领域的一个新兴研究领域，它将强化学习技术与人类反馈相结合，以训练能够学习复杂任务的个体。该方法在提高人工智能系统的性能方面显示出前景，使其在各种应用中更具有适应性和效率。">
<meta property="og:type" content="article">
<meta property="og:title" content="RLHF--基于人类反馈的强化学习">
<meta property="og:url" content="http://williamteng.github.io/2021/09/22/RLHF/index.html">
<meta property="og:site_name" content="William&#39;s Blog">
<meta property="og:description" content="基于人类反馈的强化学习（RLHF，Reinforcement Learning from Human Feedback）是人工智能（AI）领域的一个新兴研究领域，它将强化学习技术与人类反馈相结合，以训练能够学习复杂任务的个体。该方法在提高人工智能系统的性能方面显示出前景，使其在各种应用中更具有适应性和效率。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-09-22T03:12:29.000Z">
<meta property="article:modified_time" content="2023-05-20T12:32:18.519Z">
<meta property="article:author" content="William Teng">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="RLHF">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://williamteng.github.io/2021/09/22/RLHF/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>RLHF--基于人类反馈的强化学习 | William's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">William's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">William的github博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">27</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">7</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">23</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://williamteng.github.io/2021/09/22/RLHF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/author.jpg">
      <meta itemprop="name" content="William Teng">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="William's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          RLHF--基于人类反馈的强化学习
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-22 11:12:29" itemprop="dateCreated datePublished" datetime="2021-09-22T11:12:29+08:00">2021-09-22</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>基于人类反馈的强化学习（RLHF，Reinforcement Learning from Human Feedback）是人工智能（AI）领域的一个新兴研究领域，它将强化学习技术与人类反馈相结合，以训练能够学习复杂任务的个体。该方法在提高人工智能系统的性能方面显示出前景，使其在各种应用中更具有适应性和效率。</p>
<span id="more"></span>

<p>在了解RLHF之前，我们需要先知道什么是RL，强化学习（RL）是一种机器学习，在这种学习中，个体（Agent）通过与环境的互动来学习做决定。个体采取行动以实现一个特定的目标，根据其行动接受奖励或惩罚形式的反馈。随着时间的推移，个体学会了做出决策的最佳策略，以使其收到的累积奖励最大化。</p>
<h3 id="什么是强化学习Reinforcement-Learning？定义、概念、应用和挑战"><a href="#什么是强化学习Reinforcement-Learning？定义、概念、应用和挑战" class="headerlink" title="什么是强化学习Reinforcement Learning？定义、概念、应用和挑战"></a>什么是强化学习Reinforcement Learning？定义、概念、应用和挑战</h3><p>强化学习（RL）是机器学习的一个分支，重点是训练算法通过与环境的互动来做出决定。它的灵感来自于人类和动物从他们的经验中学习以实现目标的方式。在这篇文章中，我们将对强化学习、其关键概念和应用进行全面概述。</p>
<h4 id="什么是强化学习？"><a href="#什么是强化学习？" class="headerlink" title="什么是强化学习？"></a>什么是强化学习？</h4><p>强化学习（英文：Reinforcement Learning），缩写RL，是一种机器学习的方法，强调学习如何通过与环境的互动来做出决定。在强化学习中，一个主体学习在特定的环境中采取行动，以使其获得的累积奖励最大化。学习过程涉及试验和错误，主体从积极和消极反馈中学习。</p>
<p>这种学习范式起源于心理学，特别是对操作性条件反射的研究，通过这一过程，有机体学会将行动与后果联系起来。近年来，强化学习因其解决需要连续决策的复杂问题的能力而获得了巨大的吸引力。</p>
<h4 id="强化学习中的主要概念和术语"><a href="#强化学习中的主要概念和术语" class="headerlink" title="强化学习中的主要概念和术语"></a>强化学习中的主要概念和术语</h4><p>为了更好地理解强化学习，你应该熟悉以下关键概念和术语：</p>
<ul>
<li>Agent（常译为：智能体、个体、主体、玩家）： 强化学习过程中的学习者或决策者。智能体与环境互动，并采取行动以实现特定目标。</li>
<li>环境（Environment）： 智能体运作的环境。它为智能体提供观察，而智能体的行动可以影响环境的状态。</li>
<li>状态（State）： 智能体在环境中的当前状况的表示。它可以是完全或部分可观察的。</li>
<li>动作（Action）： 智能体做出的影响其与环境互动的决定。</li>
<li>奖励（Reward）： 智能体在采取一项行动后收到的即时反馈信号。奖励反映了在特定状态下采取的行动的可取性。</li>
<li>策略（Policy）： 智能体选择行动的策略，可以是确定性的或随机性的。</li>
<li>价值函数（Value function）： 一个估计智能体可以获得的预期累积奖励的函数，从一个给定的状态开始并遵循一个特定的策略。</li>
<li>Q函数（Q-function）： 一个估计智能体可以获得的预期累积奖励的函数，从一个给定的状态开始，采取一个特定的行动，然后遵循一个特定的策略。</li>
<li>探索还是利用（Exploration vs. Exploitation）： 在尝试新行动以发现其后果（探索）和选择已知可产生高回报的行动（利用）之间进行权衡。</li>
</ul>
<h4 id="强化学习的主要类型"><a href="#强化学习的主要类型" class="headerlink" title="强化学习的主要类型"></a>强化学习的主要类型</h4><p>强化学习有三种主要类型：</p>
<ul>
<li>无模型的RL：在这种方法中，智能体无法获得环境的动态模型。相反，它直接从与环境的相互作用中学习，通常是通过估计价值函数或Q-函数。</li>
<li>基于模型的RL：在这种方法中，智能体构建了一个环境动态的模型，并使用它来计划和决策。基于模型的RL可以带来更有效的学习和更好的性能，但需要精确的模型和更多的计算资源。</li>
<li>逆向RL：在这种方法中，目标是通过观察专家示范者的行为来学习他们的基本奖励函数。这在手动设计一个适当的奖励函数具有挑战性的情况下可以有所帮助。</li>
</ul>
<h4 id="强化学习的典型算法"><a href="#强化学习的典型算法" class="headerlink" title="强化学习的典型算法"></a>强化学习的典型算法</h4><p>多年来，研究人员提出了各种强化学习算法，其中最引人注目的算法包括：</p>
<ul>
<li>价值迭代（Value Iteration）： 一种动态编程技术，迭代更新价值函数，直到它收敛到最佳价值函数。</li>
<li>Q-learning： 一种无模型、非策略性的算法，通过迭代更新其基于观察到的过渡和奖励的估计值来学习最佳的Q-函数。</li>
<li>SARSA： 一种无模型的策略性算法，通过基于当前策略所采取的行动更新其估计值来学习Q函数。</li>
<li>深度Q网络（DQN）： Q-learning的扩展，使用深度神经网络来近似Q-function，使RL能够扩展到高维状态空间。</li>
<li>策略梯度算法（Policy Gradient Methods）： 一系列的算法，通过基于预期累积奖励的梯度调整其参数来直接优化策略。</li>
<li>演员评判方法（Actor-Critic Methods）： 一类算法，通过保持对策略（演员）和价值函数（评判者）的单独估计，结合基于价值和基于策略的方法。</li>
<li>近端策略优化（PPO）： 一种策略梯度方法，通过使用信任区域优化方法平衡探索和开发。</li>
</ul>
<h4 id="强化学习的应用场景"><a href="#强化学习的应用场景" class="headerlink" title="强化学习的应用场景"></a>强化学习的应用场景</h4><h5 id="机器人学和动作控制"><a href="#机器人学和动作控制" class="headerlink" title="机器人学和动作控制"></a>机器人学和动作控制</h5><p>强化学习已经成功地应用于机器人领域，使机器人能够学习复杂的任务，如抓取物体、行走和飞行。研究人员已经用RL教机器人适应新环境或从损坏中自主恢复。其他应用包括机器人手臂的优化控制和多机器人合作系统，其中多个机器人一起工作来完成任务。</p>
<h5 id="人机游戏"><a href="#人机游戏" class="headerlink" title="人机游戏"></a>人机游戏</h5><p>强化学习一直是开发能够以超人水平玩游戏的玩家的重要力量。AlphaGo和DeepMind的后续版本已经证明了RL在掌握围棋游戏方面的力量，这在以前被认为是人工智能不可能做到的。RL也被用来训练能玩雅达利游戏、国际象棋、扑克和其他复杂游戏的玩家。</p>
<h5 id="自动驾驶"><a href="#自动驾驶" class="headerlink" title="自动驾驶"></a>自动驾驶</h5><p>强化学习的最有前途的应用之一是在开发自动驾驶汽车方面。强化学习主体可以学习导航复杂的交通场景，做出智能决定以避免碰撞，并优化燃料消耗。研究人员还在探索多主体强化学习，以模拟多辆车之间的互动，并改善交通流量。</p>
<h5 id="金融量化交易"><a href="#金融量化交易" class="headerlink" title="金融量化交易"></a>金融量化交易</h5><p>强化学习已被用于优化交易策略，管理投资组合，以及预测股票价格。考虑到交易成本和市场波动，RL智能体可以学习通过对购买和出售股票做出明智的决定来实现利润最大化。此外，RL可用于算法交易，智能体学习有效地执行订单，以尽量减少市场影响和降低交易成本。</p>
<h5 id="医疗保健"><a href="#医疗保健" class="headerlink" title="医疗保健"></a>医疗保健</h5><p>在医疗保健方面，RL可以应用于个性化医疗，其目标是根据个别病人的独特特征，为他们找到最佳的治疗方案。RL还可以用来优化手术的安排，管理资源的分配，并提高医疗程序的效率。</p>
<h4 id="强化学习面临的挑战"><a href="#强化学习面临的挑战" class="headerlink" title="强化学习面临的挑战"></a>强化学习面临的挑战</h4><h5 id="样本效率"><a href="#样本效率" class="headerlink" title="样本效率"></a>样本效率</h5><p>强化学习的最大挑战之一是需要大量的数据来训练智能体。这可能很耗时，而且计算成本很高，限制了RL在现实世界场景中的适用性。研究人员正在努力开发更有样本效率的算法，使智能体能够从与环境的较少互动中学习。</p>
<h5 id="探索和利用"><a href="#探索和利用" class="headerlink" title="探索和利用"></a>探索和利用</h5><p>平衡探索（尝试新的行动以发现其效果）和利用（使用最知名的行动）是强化学习的一个基本挑战。不充分的探索可能导致次优策略，而过度的探索则会浪费宝贵的资源。开发能够有效平衡探索和利用的算法是一个活跃的研究领域。</p>
<h5 id="迁移学习和概括"><a href="#迁移学习和概括" class="headerlink" title="迁移学习和概括"></a>迁移学习和概括</h5><p>训练RL智能体将其学到的知识推广到新的任务和环境中是一个关键的挑战。迁移学习，一种旨在将在一个任务中获得的知识转移到另一个相关任务中的方法，是解决这一挑战的一个越来越流行的方法。研究人员正在探索如何使RL智能体更具有适应性，能够将其知识转移到广泛的任务和环境中。</p>
<h5 id="安全性和稳健性"><a href="#安全性和稳健性" class="headerlink" title="安全性和稳健性"></a>安全性和稳健性</h5><p>确保RL智能体的安全性和稳健性是至关重要的，特别是在自动驾驶汽车和医疗保健等应用中，错误会带来严重后果。研究人员正在努力开发将安全约束纳入学习过程的方法，使智能体对对抗性攻击更加稳健，能够处理不确定或不完整的信息。</p>
<h3 id="基于人类反馈的强化学习"><a href="#基于人类反馈的强化学习" class="headerlink" title="基于人类反馈的强化学习"></a>基于人类反馈的强化学习</h3><p>RLHF是一个将强化学习与人类反馈相结合的框架，以提高个体（Agent）在学习复杂任务中的表现。在RLHF中，人类通过提供反馈参与学习过程，帮助个体更好地理解任务，更有效地学习最优策略。将人类反馈纳入强化学习可以帮助克服与传统RL技术相关的一些挑战。人的反馈可以用来提供指导，纠正错误，并提供关于环境和任务的额外信息，而这些信息可能是个体（Agent）自己难以学习的。一些可以纳入RL的人类反馈的方式包括：</p>
<ul>
<li>提供专家示范： 人类专家可以示范正确的行为，个体可以通过模仿或利用示范与强化学习技术相结合来学习。</li>
<li>塑造奖励功能： 人类的反馈可以用来修改奖励功能，使其更有信息量，并与期望的行为更好地保持一致。</li>
<li>提供纠正性反馈： 人类可以在训练期间向个体提供纠正性反馈，使其从错误中学习并改善其表现。</li>
</ul>
<h3 id="RLHF的应用"><a href="#RLHF的应用" class="headerlink" title="RLHF的应用"></a>RLHF的应用</h3><p>RLHF已在不同领域的各种应用中显示出前景，如：</p>
<ul>
<li>智能机器人： RLHF可以用来训练机器人系统，使其以高精确度和高适应性完成复杂的任务，如操纵、运动和导航。</li>
<li>自动驾驶： RLHF可以通过纳入人类对驾驶行为和决策的反馈，帮助自主车辆学习安全和高效的驾驶策略。</li>
<li>医疗保健： RLHF可以应用于训练人工智能系统，用于个性化的治疗计划、药物发现和其他医疗应用，在这些方面人类的专业知识是至关重要的。</li>
<li>学习教育： RLHF可用于开发智能辅导系统，以适应个体学习者的需求，并根据人类的反馈提供个性化的指导。</li>
</ul>
<h3 id="RLHF的挑战"><a href="#RLHF的挑战" class="headerlink" title="RLHF的挑战"></a>RLHF的挑战</h3><ul>
<li>数据效率： 收集人类的反馈意见可能很费时和昂贵，因此，开发能够在有限的反馈意见下有效学习的方法很重要。</li>
<li>人类的偏见和不一致：人类的反馈可能容易出现偏见和不一致，这可能会影响个体的学习过程和表现。</li>
<li>可扩展性： RLHF方法需要可扩展到高维的状态和行动空间，以及复杂的环境，以适用于现实世界的任务</li>
<li>奖励的模糊性： 设计一个能准确代表所需行为的奖励函数是很有挑战性的，尤其是在包含人类反馈的时候。</li>
<li>可转移性： 经过RLHF训练的个体应该能够将他们学到的技能转移到新的任务、环境或情况中。开发促进转移学习和领域适应的方法对于实际应用是至关重要的。</li>
<li>安全性和稳健性： 确保RLHF个体是安全的，对不确定性、对抗性攻击和模型的错误规范是至关重要的，特别是在安全关键的应用中。</li>
</ul>
<p>基于人类反馈的强化学习（RLHF）是一个令人兴奋的研究领域，它结合了强化学习和人类专业知识的优势，以训练能够学习复杂任务的人工智能个体。通过将人类反馈纳入学习过程，RLHF有可能提高人工智能系统的性能、适应性和效率，包括机器人、自动驾驶汽车、医疗保健和教育等各种应用。</p>

    </div>

    
    
    
      

      <div>
        
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

        
      </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
              <a href="/tags/RLHF/" rel="tag"># RLHF</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/09/09/GreenFinance/" rel="prev" title="如火如荼的碳减排与绿色金融">
      <i class="fa fa-chevron-left"></i> 如火如荼的碳减排与绿色金融
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/10/12/GAN/" rel="next" title="生成式对抗网络（Generative Adversarial Network）">
      生成式对抗网络（Generative Adversarial Network） <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0Reinforcement-Learning%EF%BC%9F%E5%AE%9A%E4%B9%89%E3%80%81%E6%A6%82%E5%BF%B5%E3%80%81%E5%BA%94%E7%94%A8%E5%92%8C%E6%8C%91%E6%88%98"><span class="nav-number">1.</span> <span class="nav-text">什么是强化学习Reinforcement Learning？定义、概念、应用和挑战</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="nav-number">1.1.</span> <span class="nav-text">什么是强化学习？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%B8%BB%E8%A6%81%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD"><span class="nav-number">1.2.</span> <span class="nav-text">强化学习中的主要概念和术语</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%B8%BB%E8%A6%81%E7%B1%BB%E5%9E%8B"><span class="nav-number">1.3.</span> <span class="nav-text">强化学习的主要类型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%85%B8%E5%9E%8B%E7%AE%97%E6%B3%95"><span class="nav-number">1.4.</span> <span class="nav-text">强化学习的典型算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">1.5.</span> <span class="nav-text">强化学习的应用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%AD%A6%E5%92%8C%E5%8A%A8%E4%BD%9C%E6%8E%A7%E5%88%B6"><span class="nav-number">1.5.1.</span> <span class="nav-text">机器人学和动作控制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BA%BA%E6%9C%BA%E6%B8%B8%E6%88%8F"><span class="nav-number">1.5.2.</span> <span class="nav-text">人机游戏</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6"><span class="nav-number">1.5.3.</span> <span class="nav-text">自动驾驶</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%87%91%E8%9E%8D%E9%87%8F%E5%8C%96%E4%BA%A4%E6%98%93"><span class="nav-number">1.5.4.</span> <span class="nav-text">金融量化交易</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8C%BB%E7%96%97%E4%BF%9D%E5%81%A5"><span class="nav-number">1.5.5.</span> <span class="nav-text">医疗保健</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%9D%A2%E4%B8%B4%E7%9A%84%E6%8C%91%E6%88%98"><span class="nav-number">1.6.</span> <span class="nav-text">强化学习面临的挑战</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A0%B7%E6%9C%AC%E6%95%88%E7%8E%87"><span class="nav-number">1.6.1.</span> <span class="nav-text">样本效率</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8E%A2%E7%B4%A2%E5%92%8C%E5%88%A9%E7%94%A8"><span class="nav-number">1.6.2.</span> <span class="nav-text">探索和利用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%A6%82%E6%8B%AC"><span class="nav-number">1.6.3.</span> <span class="nav-text">迁移学习和概括</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%89%E5%85%A8%E6%80%A7%E5%92%8C%E7%A8%B3%E5%81%A5%E6%80%A7"><span class="nav-number">1.6.4.</span> <span class="nav-text">安全性和稳健性</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E4%BA%BA%E7%B1%BB%E5%8F%8D%E9%A6%88%E7%9A%84%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.</span> <span class="nav-text">基于人类反馈的强化学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RLHF%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">3.</span> <span class="nav-text">RLHF的应用</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RLHF%E7%9A%84%E6%8C%91%E6%88%98"><span class="nav-number">4.</span> <span class="nav-text">RLHF的挑战</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="William Teng"
      src="/images/author.jpg">
  <p class="site-author-name" itemprop="name">William Teng</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">23</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">27</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/WilliamTeng" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;WilliamTeng" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:william.teng@aliyun.com" title="E-Mail → mailto:william.teng@aliyun.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2016 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">William Teng</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
